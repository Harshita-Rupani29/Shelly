# Environment Variables Template for {{projectName}}
# Copy this file to .env and fill in your actual values
# Generated: {{date}} | Owner: {{owner}}

# ================================
# APPLICATION CONFIGURATION
# ================================

# Node Environment (development, production, test)
NODE_ENV=development

# Application Port
PORT=3000

# Application URL (for callbacks, links, etc.)
APP_URL=http://localhost:3000

# ================================
# DATABASE CONFIGURATION
# ================================

# PostgreSQL (recommended)
# DATABASE_URL=postgresql://YOUR_USER:YOUR_PASSWORD@localhost:5432/{{repoName}}_dev

# MongoDB
# MONGODB_URI=mongodb://localhost:27017/{{repoName}}_dev

# Redis (for caching/sessions)
# REDIS_URL=redis://localhost:6379

# ================================
# AI/ML CONFIGURATION - MULTI-PROVIDER
# ================================
# Neurolink supports multiple AI providers with automatic fallback
# Configure one or more providers below

# --------------------------------
# PROVIDER SELECTION & ROUTING
# --------------------------------

# Primary AI provider to use
# Options: openai, anthropic, vertex, bedrock, azure, huggingface, ollama, mistral, cohere
AI_PROVIDER=openai

# Enable automatic fallback to other providers on failure
AI_FALLBACK_ENABLED=true

# Fallback chain (comma-separated, order matters)
AI_FALLBACK_CHAIN=openai,anthropic,vertex

# Default model for text generation
AI_DEFAULT_MODEL=gpt-4o-mini

# Default model for embeddings
AI_EMBEDDING_MODEL=text-embedding-3-small

# Request timeout in milliseconds
AI_TIMEOUT=30000

# Maximum retries on failure
AI_MAX_RETRIES=3

# --------------------------------
# OPENAI CONFIGURATION
# --------------------------------
# https://platform.openai.com/api-keys

# API Key (required for OpenAI)
# OPENAI_API_KEY=sk-...

# Organization ID (optional)
# OPENAI_ORG_ID=org-...

# Project ID (optional)
# OPENAI_PROJECT_ID=proj_...

# API Base URL (for proxies or compatible APIs)
# OPENAI_BASE_URL=https://api.openai.com/v1

# Model selection (defaults shown)
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# --------------------------------
# ANTHROPIC CLAUDE CONFIGURATION
# --------------------------------
# https://console.anthropic.com/

# API Key (required for Anthropic)
# ANTHROPIC_API_KEY=sk-ant-...

# API Base URL (optional)
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# Model selection
# Available: claude-opus-4-5-20251101, claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001
# ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# Maximum tokens for response
# ANTHROPIC_MAX_TOKENS=4096

# --------------------------------
# GOOGLE VERTEX AI CONFIGURATION
# --------------------------------
# https://cloud.google.com/vertex-ai/docs/start/cloud-environment

# Authentication Method 1: Service Account Key File (recommended for development)
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Authentication Method 2: Base64-encoded Service Account Key (for CI/CD)
# GOOGLE_SERVICE_ACCOUNT_KEY=BASE64_ENCODED_SERVICE_ACCOUNT_JSON

# Authentication Method 3: Application Default Credentials (for GCP environments)
# Uses automatic detection when running on GCP

# GCP Project ID (required)
# GOOGLE_CLOUD_PROJECT_ID=your-project-id

# Vertex AI Location/Region
# VERTEX_LOCATION=us-central1

# Model selection
# Available: gemini-2.5-flash, gemini-2.5-pro, gemini-1.5-flash, gemini-1.5-pro
# VERTEX_MODEL=gemini-2.5-flash

# --------------------------------
# AMAZON BEDROCK CONFIGURATION
# --------------------------------
# https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html

# AWS Credentials (or use AWS CLI profile)
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...

# AWS Session Token (for temporary credentials)
# AWS_SESSION_TOKEN=...

# AWS Region for Bedrock
# AWS_BEDROCK_REGION=us-east-1

# Model ID for Bedrock
# Available: anthropic.claude-v2, anthropic.claude-instant-v1,
#            amazon.titan-text-express-v1, amazon.titan-embed-text-v1,
#            ai21.j2-ultra-v1, cohere.command-text-v14
# BEDROCK_MODEL_ID=anthropic.claude-v2

# Inference profile (optional)
# BEDROCK_INFERENCE_PROFILE=

# --------------------------------
# AZURE OPENAI CONFIGURATION
# --------------------------------
# https://azure.microsoft.com/en-us/products/ai-services/openai-service

# Azure OpenAI Endpoint
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# Azure OpenAI API Key
# AZURE_OPENAI_API_KEY=...

# Azure OpenAI API Version
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Deployment name (your deployed model name)
# AZURE_OPENAI_DEPLOYMENT=gpt-4

# Embedding deployment name
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002

# --------------------------------
# HUGGING FACE CONFIGURATION
# --------------------------------
# https://huggingface.co/settings/tokens

# Hugging Face API Token
# HUGGINGFACE_API_TOKEN=hf_...

# Model ID (for Inference API)
# HUGGINGFACE_MODEL=meta-llama/Llama-3.3-70B-Instruct

# Embedding Model
# HUGGINGFACE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Inference Endpoint (for dedicated endpoints)
# HUGGINGFACE_ENDPOINT=https://your-endpoint.endpoints.huggingface.cloud

# --------------------------------
# OLLAMA CONFIGURATION (LOCAL AI)
# --------------------------------
# https://ollama.ai

# Ollama Base URL (default: http://localhost:11434)
# OLLAMA_HOST=http://localhost:11434

# Default model (must be pulled first: ollama pull llama3)
# Available: llama3, llama2, mistral, codellama, mixtral, phi, neural-chat
# OLLAMA_MODEL=llama3

# Request timeout for local inference (longer for larger models)
# OLLAMA_TIMEOUT=120000

# Number of GPU layers to use (-1 for auto)
# OLLAMA_GPU_LAYERS=-1

# --------------------------------
# MISTRAL AI CONFIGURATION
# --------------------------------
# https://console.mistral.ai/

# Mistral API Key
# MISTRAL_API_KEY=...

# Mistral API Base URL
# MISTRAL_BASE_URL=https://api.mistral.ai

# Model selection
# Available: mistral-large-latest, mistral-medium-latest, mistral-small-latest
# MISTRAL_MODEL=mistral-large-latest

# --------------------------------
# COHERE CONFIGURATION
# --------------------------------
# https://dashboard.cohere.com/api-keys

# Cohere API Key
# COHERE_API_KEY=...

# Model selection
# Available: command, command-light, command-nightly
# COHERE_MODEL=command

# Embedding Model
# COHERE_EMBED_MODEL=embed-english-v3.0

# ================================
# MCP (MODEL CONTEXT PROTOCOL)
# ================================
# Configuration for MCP server connections

# Enable MCP servers
MCP_ENABLED=true

# MCP configuration file path
# MCP_CONFIG_PATH=.mcp-servers.json

# Default MCP timeout
# MCP_TIMEOUT=30000

# MCP servers to auto-connect (comma-separated)
# MCP_AUTO_CONNECT=filesystem,github

# ================================
# NEUROLINK CONFIGURATION
# ================================
# Neurolink-specific settings

# Enable AI features in Shelly
SHELLY_AI_ENABLED=true

# Memory Bank location
# MEMORY_BANK_PATH=docs/memory-bank

# Workflow templates directory
# WORKFLOWS_PATH=.ai/workflows

# Enable AI-powered code review
# AI_CODE_REVIEW_ENABLED=true

# AI analysis depth (quick, standard, comprehensive)
# AI_ANALYSIS_DEPTH=standard

# ================================
# API KEYS & EXTERNAL SERVICES
# ================================

# GitHub Token (for GitHub API operations)
# GITHUB_TOKEN=ghp_...

# GitHub Enterprise (if applicable)
# GITHUB_API_URL=https://api.github.com
# GITHUB_ENTERPRISE_URL=https://github.your-company.com/api/v3

# Stripe (payments)
# STRIPE_SECRET_KEY=sk_test_...
# STRIPE_PUBLISHABLE_KEY=pk_test_...
# STRIPE_WEBHOOK_SECRET=whsec_...

# SendGrid (email)
# SENDGRID_API_KEY=SG...

# Twilio (SMS)
# TWILIO_ACCOUNT_SID=AC...
# TWILIO_AUTH_TOKEN=...

# ================================
# AUTHENTICATION & SECURITY
# ================================

# JWT Configuration
# JWT_SECRET=your_jwt_secret_here_use_secure_random_string
# JWT_EXPIRES_IN=7d
# JWT_REFRESH_EXPIRES_IN=30d

# Session Configuration
# SESSION_SECRET=your_session_secret_here
# SESSION_MAX_AGE=86400

# OAuth Providers (if using social login)
# GOOGLE_CLIENT_ID=...
# GOOGLE_CLIENT_SECRET=...
# GITHUB_CLIENT_ID=...
# GITHUB_CLIENT_SECRET=...

# ================================
# CLOUD SERVICES
# ================================

# AWS Configuration (general)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_REGION=us-east-1
# AWS_S3_BUCKET=your_bucket_name

# Google Cloud Storage
# GCS_BUCKET=your_gcs_bucket

# Cloudflare
# CLOUDFLARE_API_TOKEN=...
# CLOUDFLARE_ZONE_ID=...

# ================================
# DEVELOPMENT & DEBUGGING
# ================================

# Debug Mode
DEBUG=false
# SHELLY_DEBUG=false
# SHELL_OVERRIDE=bash

# Enable verbose logging
VERBOSE=false

# Logging Level (error, warn, info, debug, trace)
LOG_LEVEL=info

# Log format (json, pretty)
# LOG_FORMAT=pretty

# Enable request logging
# LOG_REQUESTS=true

# Enable Swagger/OpenAPI docs
# ENABLE_SWAGGER=true

# Enable CORS
# ENABLE_CORS=true

# ================================
# PERFORMANCE & OPTIMIZATION
# ================================

# Enable response caching
# CACHE_ENABLED=true
# CACHE_TTL=3600

# Rate limiting
# RATE_LIMIT_WINDOW_MS=60000
# RATE_LIMIT_MAX_REQUESTS=100

# Request body size limit
# MAX_BODY_SIZE=10mb

# Concurrent AI request limit
# AI_CONCURRENCY_LIMIT=5

# ================================
# MONITORING & OBSERVABILITY
# ================================

# Sentry (error tracking)
# SENTRY_DSN=https://...
# SENTRY_ENVIRONMENT=development

# DataDog
# DD_API_KEY=...
# DD_APP_KEY=...

# New Relic
# NEW_RELIC_LICENSE_KEY=...

# Prometheus metrics endpoint
# METRICS_ENABLED=true
# METRICS_PORT=9090

# ================================
# FEATURE FLAGS
# ================================

# Feature toggles for gradual rollouts
# FEATURE_NEW_UI=false
# FEATURE_BETA_API=false
# FEATURE_AI_ASSISTANT=true

# ================================
# NOTES
# ================================
#
# 1. NEVER commit .env files to version control
# 2. Keep this .env.example updated with new variables
# 3. Use strong, unique values for secrets and keys
# 4. For production, use secret managers (AWS Secrets Manager, HashiCorp Vault)
# 5. AI providers are optional - configure only what you need
# 6. Test AI provider connectivity: shelly config --test-ai
# 7. View current AI config: shelly config --show
# 8. Disable AI features: shelly config --disable-ai
#
# For more information, see:
# - Setup Guide: docs/SETUP.md
# - Getting Started: docs/GETTING_STARTED.md
# - Security Best Practices: SECURITY.md
